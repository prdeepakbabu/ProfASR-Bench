{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR Evaluation with Whisper on AgenticASR Dataset\n",
    "\n",
    "**Optimized for Apple Silicon M3 Pro with 36GB Unified Memory**\n",
    "\n",
    "This notebook demonstrates comprehensive ASR evaluation including:\n",
    "- Loading private HuggingFace dataset\n",
    "- Whisper-small model with MPS acceleration\n",
    "- N-best hypothesis generation\n",
    "- WER@1, WER@5, and SER metrics\n",
    "- Detailed error analysis\n",
    "- Domain and voice-specific breakdowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom modules\n",
    "from src.data_loader import ASRDataLoader\n",
    "from src.asr_models import ASRModelWrapper\n",
    "from src.metrics import ASRMetrics\n",
    "from src.experiment_tracker import ExperimentTracker\n",
    "from config.model_configs import get_model_config, check_memory_requirements\n",
    "from config.decoding_configs import get_decoding_config\n",
    "from config.experiment_configs import create_experiment_config, DATASET_CONFIGS\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'MPS' if torch.backends.mps.is_available() else 'CPU'}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Configuration and Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System information\n",
    "import psutil\n",
    "\n",
    "print(\"=== System Information ===\")\n",
    "print(f\"Platform: {torch.backends.mps.is_available() and 'Apple Silicon (MPS)' or 'Other'}\")\n",
    "print(f\"Total Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / (1024**3):.1f} GB\")\n",
    "print(f\"CPU Cores: {psutil.cpu_count()}\")\n",
    "\n",
    "# Check memory requirements for whisper-small\n",
    "memory_check = check_memory_requirements(\"whisper-small\", batch_size=32)\n",
    "print(\"\\n=== Memory Requirements Check ===\")\n",
    "print(f\"Model memory: {memory_check['model_memory_gb']:.1f} GB\")\n",
    "print(f\"Batch memory (32 samples): {memory_check['batch_memory_gb']:.1f} GB\")\n",
    "print(f\"Total estimated: {memory_check['total_estimated_gb']:.1f} GB\")\n",
    "print(f\"Fits in available memory: {memory_check['fits_in_36gb']}\")\n",
    "print(f\"Recommended batch size: {memory_check['recommended_batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment configuration\n",
    "experiment_config = create_experiment_config(\n",
    "    model_name=\"whisper-small\",\n",
    "    decoding_name=\"beam_search_5\",  # 5-beam search for n-best hypotheses\n",
    "    dataset_name=\"quick_test\",  # Start with quick test, change to \"full\" for complete evaluation\n",
    "    description=\"Whisper-small evaluation with 5-beam search on AgenticASR dataset\",\n",
    "    batch_size=32  # Optimal for M3 Pro\n",
    ")\n",
    "\n",
    "print(\"=== Experiment Configuration ===\")\n",
    "print(f\"Experiment ID: {experiment_config.experiment_id}\")\n",
    "print(f\"Description: {experiment_config.description}\")\n",
    "print(f\"Model: {experiment_config.model_config.name}\")\n",
    "print(f\"Decoding: {experiment_config.decoding_config.strategy.value}\")\n",
    "print(f\"Dataset: {experiment_config.dataset_config.name}\")\n",
    "print(f\"Batch size: {experiment_config.processing_config.batch_size}\")\n",
    "print(f\"Device: {experiment_config.model_config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Loading and Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Login interactively\n",
    "from huggingface_hub import login\n",
    "login()  # Enter your token when prompted\n",
    "\n",
    "# Method 2: Set environment variable\n",
    "import os\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = ASRDataLoader(\n",
    "    dataset_config=experiment_config.dataset_config,\n",
    "    processing_config=experiment_config.processing_config\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    dataset = data_loader.load_dataset()\n",
    "    print(f\"\u2713 Successfully loaded {len(dataset)} samples\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error loading dataset: {e}\")\n",
    "    print(\"\\nTrying to load from local path...\")\n",
    "    # Fallback to local dataset if available\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "stats = data_loader.get_dataset_stats()\n",
    "\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Total samples: {stats['total_samples']}\")\n",
    "print(f\"\\nDomain distribution:\")\n",
    "for domain, count in stats['domain_distribution'].items():\n",
    "    print(f\"  {domain}: {count} samples\")\n",
    "\n",
    "print(f\"\\nVoice distribution:\")\n",
    "for voice, count in stats['voice_distribution'].items():\n",
    "    print(f\"  {voice}: {count} samples\")\n",
    "\n",
    "print(f\"\\nAudio length statistics:\")\n",
    "audio_stats = stats['audio_length_stats']\n",
    "print(f\"  Mean: {audio_stats['mean']:.2f}s\")\n",
    "print(f\"  Std: {audio_stats['std']:.2f}s\")\n",
    "print(f\"  Range: {audio_stats['min']:.2f}s - {audio_stats['max']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample data\n",
    "preview = data_loader.preview_samples(n_samples=3)\n",
    "\n",
    "print(\"=== Sample Preview ===\")\n",
    "for i, sample in enumerate(preview):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  ID: {sample['utterance_id']}\")\n",
    "    print(f\"  Domain: {sample['domain']}\")\n",
    "    print(f\"  Voice: {sample['voice']}\")\n",
    "    print(f\"  Text: {sample['text']}\")\n",
    "    print(f\"  Truth: {sample['truth']}\")\n",
    "    print(f\"  Prompt: {sample['prompt']}\")\n",
    "    print(f\"  Audio length: {sample['audio_length']:.2f}s\")\n",
    "    print(f\"  ASR difficulty: {sample['asr_difficulty']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ASR model wrapper\n",
    "print(\"Initializing ASR model...\")\n",
    "asr_model = ASRModelWrapper(experiment_config.model_config)\n",
    "\n",
    "# Load model (this may take a minute)\n",
    "print(\"Loading Whisper-small model...\")\n",
    "start_time = datetime.now()\n",
    "try:\n",
    "    asr_model.load_model()\n",
    "    load_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"\u2713 Model loaded successfully in {load_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model information\n",
    "model_info = asr_model.get_model_info()\n",
    "\n",
    "print(\"=== Model Information ===\")\n",
    "print(f\"Model ID: {model_info['model_id']}\")\n",
    "print(f\"Device: {model_info['device']}\")\n",
    "print(f\"Data type: {model_info['dtype']}\")\n",
    "print(f\"Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"Model memory: {model_info['model_memory_gb']:.2f} GB\")\n",
    "print(f\"Status: {model_info['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmarking (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick benchmark with a sample audio\n",
    "print(\"Running quick benchmark...\")\n",
    "\n",
    "# Get a sample for benchmarking\n",
    "pytorch_dataset = data_loader.create_pytorch_dataset()\n",
    "sample = pytorch_dataset[0]\n",
    "sample_audio = sample['audio']\n",
    "\n",
    "# Benchmark inference\n",
    "benchmark_results = asr_model.benchmark_inference(\n",
    "    audio_array=sample_audio,\n",
    "    decoding_config=experiment_config.decoding_config,\n",
    "    n_runs=5\n",
    ")\n",
    "\n",
    "print(\"=== Benchmark Results ===\")\n",
    "print(f\"Mean inference time: {benchmark_results['mean_time']:.3f}s\")\n",
    "print(f\"Std deviation: {benchmark_results['std_time']:.3f}s\")\n",
    "print(f\"Min time: {benchmark_results['min_time']:.3f}s\")\n",
    "print(f\"Max time: {benchmark_results['max_time']:.3f}s\")\n",
    "print(f\"Real-time factor: {benchmark_results['rtf']:.2f}x\")\n",
    "print(f\"Audio duration: {benchmark_results['audio_duration']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook, reload the updated modules\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "for module_name in ['src.asr_models', 'config.decoding_configs']:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual transcription with n-best hypotheses\n",
    "sample = pytorch_dataset[89]\n",
    "sample_audio = sample['audio']\n",
    "result = asr_model.transcribe_single(\n",
    "    audio_array=sample_audio,\n",
    "    decoding_config=experiment_config.decoding_config\n",
    ")\n",
    "\n",
    "print(\"=== ASR Results ===\")\n",
    "print(f\"Best prediction: '{result['prediction']}'\")\n",
    "print(f\"Confidence score: {result['score']:.3f}\")\n",
    "print(f\"Inference time: {result['inference_time']:.3f}s\")\n",
    "\n",
    "print(\"\\n=== N-Best Hypotheses ===\")\n",
    "for i, hyp in enumerate(result['hypotheses'][:5]):  # Top 5\n",
    "    print(f\"{i+1}. '{hyp['text']}' (score: {hyp['score']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "if 'src.asr_models' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.asr_models'])\n",
    "    print(\"\u2705 Reloaded ASR models with bulletproof fix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Full Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment tracker\n",
    "tracker = ExperimentTracker()\n",
    "experiment_id = tracker.start_experiment(experiment_config)\n",
    "\n",
    "# Initialize metrics calculator\n",
    "metrics_calculator = ASRMetrics()\n",
    "\n",
    "print(f\"Started experiment: {experiment_id}\")\n",
    "print(f\"Total samples to process: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for batched processing\n",
    "dataloader = data_loader.create_dataloader(shuffle=False)\n",
    "\n",
    "print(f\"Created dataloader with {len(dataloader)} batches\")\n",
    "print(f\"Batch size: {experiment_config.processing_config.batch_size}\")\n",
    "\n",
    "# Storage for all results\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "all_hypotheses = []\n",
    "all_metadata = {\n",
    "    'utterance_id': [],\n",
    "    'domain': [],\n",
    "    'voice': [],\n",
    "    'asr_difficulty': []\n",
    "}\n",
    "\n",
    "# Process batches\n",
    "total_samples = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Processing batches\")):\n",
    "        # Extract batch data\n",
    "        audio_arrays = batch['audio']\n",
    "        references = batch['truth']  # Use 'truth' for evaluation\n",
    "        \n",
    "        # Transcribe batch\n",
    "        batch_results = asr_model.transcribe_batch(\n",
    "            audio_arrays=audio_arrays,\n",
    "            decoding_config=experiment_config.decoding_config\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        batch_predictions = batch_results['predictions']\n",
    "        batch_hypotheses = batch_results['all_hypotheses']\n",
    "        \n",
    "        # Convert hypotheses format for metrics\n",
    "        batch_hypotheses_text = []\n",
    "        for hyp_list in batch_hypotheses:\n",
    "            hyp_texts = [h['text'] for h in hyp_list]\n",
    "            batch_hypotheses_text.append(hyp_texts)\n",
    "        \n",
    "        # Calculate batch metrics\n",
    "        batch_metrics = metrics_calculator.calculate_batch_metrics(\n",
    "            references=references,\n",
    "            hypotheses_list=batch_hypotheses_text,\n",
    "            n_values=experiment_config.compute_wer_at_n,\n",
    "            domains=batch['domain'],\n",
    "            voices=batch['voice']\n",
    "        )\n",
    "        \n",
    "        # Log batch results\n",
    "        tracker.log_batch_results(\n",
    "            batch_predictions=batch_predictions,\n",
    "            batch_references=references,\n",
    "            batch_hypotheses=batch_hypotheses_text,\n",
    "            batch_metadata={\n",
    "                'utterance_id': batch['utterance_id'],\n",
    "                'domain': batch['domain'],\n",
    "                'voice': batch['voice'],\n",
    "                'asr_difficulty': batch['asr_difficulty']\n",
    "            },\n",
    "            batch_metrics=batch_metrics\n",
    "        )\n",
    "        \n",
    "        # Accumulate results\n",
    "        all_predictions.extend(batch_predictions)\n",
    "        all_references.extend(references)\n",
    "        all_hypotheses.extend(batch_hypotheses_text)\n",
    "        \n",
    "        for key in all_metadata.keys():\n",
    "            all_metadata[key].extend(batch[key])\n",
    "        \n",
    "        total_samples += len(batch_predictions)\n",
    "        \n",
    "        # Progress update\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
    "            samples_per_sec = total_samples / elapsed\n",
    "            print(f\"Processed {total_samples} samples in {elapsed:.1f}s ({samples_per_sec:.1f} samples/sec)\")\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            asr_model.clear_cache()\n",
    "\n",
    "    print(f\"\\n\u2713 Completed processing {total_samples} samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error during evaluation: {e}\")\n",
    "    tracker.finish_experiment(success=False, error_message=str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final comprehensive metrics\n",
    "print(\"Calculating final metrics...\")\n",
    "\n",
    "final_metrics = metrics_calculator.calculate_batch_metrics(\n",
    "    references=all_references,\n",
    "    hypotheses_list=all_hypotheses,\n",
    "    n_values=experiment_config.compute_wer_at_n,\n",
    "    domains=all_metadata['domain'],\n",
    "    voices=all_metadata['voice']\n",
    ")\n",
    "\n",
    "# Update experiment with final metrics\n",
    "tracker.update_experiment_metrics(final_metrics)\n",
    "\n",
    "print(\"=== Overall Results ===\")\n",
    "overall = final_metrics['overall']\n",
    "\n",
    "for n in experiment_config.compute_wer_at_n:\n",
    "    wer_key = f\"wer_{n}\"\n",
    "    ser_key = f\"ser_{n}\"\n",
    "    \n",
    "    if wer_key in overall:\n",
    "        wer_stats = overall[wer_key]\n",
    "        print(f\"WER@{n}: {wer_stats['mean']:.3f} (\u00b1{wer_stats['std']:.3f})\")\n",
    "    \n",
    "    if ser_key in overall:\n",
    "        ser_stats = overall[ser_key]\n",
    "        print(f\"SER@{n}: {ser_stats['mean']:.3f} (\u00b1{ser_stats['std']:.3f})\")\n",
    "\n",
    "print(f\"\\nTotal samples: {final_metrics['total_samples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Domain and Voice Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain breakdown\n",
    "print(\"=== Domain Breakdown ===\")\n",
    "domain_metrics = final_metrics['domain_breakdown']\n",
    "\n",
    "domain_results = []\n",
    "for domain, metrics in domain_metrics.items():\n",
    "    wer_1 = metrics.get('wer_1', {}).get('mean', 0) * 100\n",
    "    wer_5 = metrics.get('wer_5', {}).get('mean', 0) * 100\n",
    "    ser_1 = metrics.get('ser_1', {}).get('mean', 0) * 100\n",
    "    \n",
    "    domain_results.append({\n",
    "        'Domain': domain,\n",
    "        'WER@1 (%)': f\"{wer_1:.1f}\",\n",
    "        'WER@5 (%)': f\"{wer_5:.1f}\", \n",
    "        'SER@1 (%)': f\"{ser_1:.1f}\"\n",
    "    })\n",
    "    \n",
    "domain_df = pd.DataFrame(domain_results)\n",
    "print(domain_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice breakdown\n",
    "print(\"=== Voice Breakdown ===\")\n",
    "voice_metrics = final_metrics['voice_breakdown']\n",
    "\n",
    "voice_results = []\n",
    "for voice, metrics in voice_metrics.items():\n",
    "    wer_1 = metrics.get('wer_1', {}).get('mean', 0) * 100\n",
    "    wer_5 = metrics.get('wer_5', {}).get('mean', 0) * 100\n",
    "    ser_1 = metrics.get('ser_1', {}).get('mean', 0) * 100\n",
    "    \n",
    "    voice_results.append({\n",
    "        'Voice': voice,\n",
    "        'WER@1 (%)': f\"{wer_1:.1f}\",\n",
    "        'WER@5 (%)': f\"{wer_5:.1f}\",\n",
    "        'SER@1 (%)': f\"{ser_1:.1f}\"\n",
    "    })\n",
    "\n",
    "voice_df = pd.DataFrame(voice_results)\n",
    "print(voice_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed error report\n",
    "print(\"Generating error analysis...\")\n",
    "\n",
    "error_report = metrics_calculator.generate_error_report(\n",
    "    references=all_references,\n",
    "    hypotheses_list=all_hypotheses,\n",
    "    utterance_ids=all_metadata['utterance_id'],\n",
    "    domains=all_metadata['domain'],\n",
    "    voices=all_metadata['voice'],\n",
    "    worst_n=5\n",
    ")\n",
    "\n",
    "print(\"=== Error Analysis Summary ===\")\n",
    "print(f\"Total samples: {error_report['total_samples']}\")\n",
    "print(f\"Perfect samples (WER=0): {error_report['perfect_samples']}\")\n",
    "print(f\"Samples with errors: {error_report['samples_with_errors']}\")\n",
    "print(f\"Perfect accuracy: {error_report['perfect_samples']/error_report['total_samples']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show worst errors\n",
    "print(\"=== Top 5 Worst Errors ===\")\n",
    "for i, error in enumerate(error_report['worst_errors'][:5]):\n",
    "    print(f\"\\n{i+1}. WER: {error['wer']:.3f} | Domain: {error['domain']} | Voice: {error['voice']}\")\n",
    "    print(f\"   Reference: {error['reference']}\")\n",
    "    print(f\"   Hypothesis: {error['hypothesis']}\")\n",
    "    print(f\"   Errors: {error['substitutions']}S, {error['deletions']}D, {error['insertions']}I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup visualization\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ASR Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Domain comparison\n",
    "domain_wer = [float(r['WER@1 (%)']) for r in domain_results]\n",
    "domain_names = [r['Domain'] for r in domain_results]\n",
    "\n",
    "axes[0,0].bar(domain_names, domain_wer, color='skyblue', alpha=0.7)\n",
    "axes[0,0].set_title('WER@1 by Domain')\n",
    "axes[0,0].set_ylabel('WER@1 (%)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Voice comparison\n",
    "voice_wer = [float(r['WER@1 (%)']) for r in voice_results]\n",
    "voice_names = [r['Voice'] for r in voice_results]\n",
    "\n",
    "axes[0,1].bar(voice_names, voice_wer, color='lightcoral', alpha=0.7)\n",
    "axes[0,1].set_title('WER@1 by Voice')\n",
    "axes[0,1].set_ylabel('WER@1 (%)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. WER@1 vs WER@5 comparison\n",
    "wer_1_overall = overall['wer_1']['mean'] * 100\n",
    "wer_5_overall = overall['wer_5']['mean'] * 100\n",
    "ser_1_overall = overall['ser_1']['mean'] * 100\n",
    "\n",
    "metrics_comparison = ['WER@1', 'WER@5', 'SER@1']\n",
    "values = [wer_1_overall, wer_5_overall, ser_1_overall]\n",
    "\n",
    "axes[1,0].bar(metrics_comparison, values, color=['orange', 'green', 'purple'], alpha=0.7)\n",
    "axes[1,0].set_title('Overall Metrics Comparison')\n",
    "axes[1,0].set_ylabel('Error Rate (%)')\n",
    "\n",
    "# 4. Error distribution\n",
    "wer_dist = [sample['wer_1'] for sample in final_metrics['sample_metrics']]\n",
    "axes[1,1].hist(wer_dist, bins=20, color='gold', alpha=0.7)\n",
    "axes[1,1].set_title('WER@1 Distribution')\n",
    "axes[1,1].set_xlabel('WER@1')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Experiment Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish experiment\n",
    "print(\"Finishing experiment...\")\n",
    "final_experiment_id = tracker.finish_experiment(success=True)\n",
    "\n",
    "print(f\"\u2713 Experiment completed successfully: {final_experiment_id}\")\n",
    "\n",
    "# Generate experiment report\n",
    "experiment_report = tracker.generate_experiment_report(final_experiment_id)\n",
    "\n",
    "print(\"\\n=== Experiment Summary ===\")\n",
    "print(f\"Experiment ID: {experiment_report['experiment_id']}\")\n",
    "print(f\"Description: {experiment_report['description']}\")\n",
    "print(f\"Duration: {experiment_report['duration_seconds']:.1f} seconds\")\n",
    "print(f\"Status: {experiment_report['status']}\")\n",
    "\n",
    "if 'performance' in experiment_report:\n",
    "    perf = experiment_report['performance']\n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Samples per second: {perf['samples_per_second']:.2f}\")\n",
    "    print(f\"  Seconds per sample: {perf['seconds_per_sample']:.3f}\")\n",
    "\n",
    "print(f\"\\nResults saved to: experiments/results/\")\n",
    "print(f\"- Config: experiment_logs/{final_experiment_id}_config.json\")\n",
    "print(f\"- Results: experiment_logs/{final_experiment_id}_results.json\")\n",
    "print(f\"- Predictions: predictions/{final_experiment_id}_predictions.json\")\n",
    "print(f\"- Metrics: metrics/{final_experiment_id}_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Next Steps ===\")\n",
    "print(\"\")\n",
    "print(\"1. **Full Dataset Evaluation**:\")\n",
    "print(\"   - Change dataset_name to 'full' in experiment config\")\n",
    "print(\"   - Run evaluation on all 3,200 samples\")\n",
    "print(\"\")\n",
    "print(\"2. **Model Comparison**:\")\n",
    "print(\"   - Try whisper-base, whisper-medium for comparison\")\n",
    "print(\"   - Evaluate distil-whisper models for speed\")\n",
    "print(\"\")\n",
    "print(\"3. **Decoding Strategy Comparison**:\")\n",
    "print(\"   - Compare greedy vs beam search vs nucleus sampling\")\n",
    "print(\"   - Evaluate different beam sizes (3, 5, 10)\")\n",
    "print(\"\")\n",
    "print(\"4. **Domain-Specific Analysis**:\")\n",
    "print(\"   - Run separate evaluations for each domain\")\n",
    "print(\"   - Analyze domain-specific error patterns\")\n",
    "print(\"\")\n",
    "print(\"5. **Hyperparameter Tuning**:\")\n",
    "print(\"   - Experiment with temperature settings\")\n",
    "print(\"   - Try different length penalties\")\n",
    "print(\"\")\n",
    "print(\"6. **Error Analysis**:\")\n",
    "print(\"   - Analyze worst-performing samples\")\n",
    "print(\"   - Study correlation with ASR difficulty scores\")\n",
    "print(\"\")\n",
    "print(\"=== Usage Tips ===\")\n",
    "print(\"\")\n",
    "print(\"- Use tracker.list_experiments() to see all experiments\")\n",
    "print(\"- Use tracker.compare_experiments([id1, id2]) for comparisons\")\n",
    "print(\"- Modify batch_size based on available memory\")\n",
    "print(\"- Monitor MPS memory usage with Activity Monitor\")\n",
    "print(\"\")\n",
    "print(\"=== Performance on M3 Pro ===\")\n",
    "print(f\"Expected performance with whisper-small:\")\n",
    "print(f\"- ~32 samples per batch: 3-6 seconds\")\n",
    "print(f\"- Full dataset (3,200 samples): 10-20 minutes\")\n",
    "print(f\"- Memory usage: ~8-12 GB\")\n",
    "print(f\"- Real-time factor: ~0.1-0.3x (much faster than real-time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
